{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "066e2096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "1cddc4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "df1 = pd.read_csv('/home/wahba/Documents/cicid/cicids2017/original/csv/TrafficLabelling /Wednesday-workingHours.pcap_ISCX.csv')\n",
    "df2 = pd.read_csv('/home/wahba/Documents/cicid/CICFlowMeter/data/daily/2025-11-21_Flow.csv')\n",
    "df3 = pd.read_csv('/home/wahba/Documents/nids3/tests/flow/flow_tuesday_flow_generation4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "826ed2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning column names\n",
    "col_names = {col: col.strip() for col in df1.columns}\n",
    "df1.rename(columns=col_names, inplace=True)\n",
    "col_names = {col: col.strip() for col in df2.columns}\n",
    "df2.rename(columns=col_names, inplace=True)\n",
    "col_names = {col: col.strip() for col in df3.columns}\n",
    "df3.rename(columns=col_names, inplace=True)\n",
    "\n",
    "del col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c595aa90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "BENIGN              440031\n",
       "DoS Hulk            231073\n",
       "DoS GoldenEye        10293\n",
       "DoS slowloris         5796\n",
       "DoS Slowhttptest      5499\n",
       "Heartbleed              11\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "5fa36205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out only DoS slowhttptest attacks\n",
    "df1_dos = df1[df1['Label'] == 'DoS Slowhttptest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e03e17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_dos['flow_key'] = (\n",
    "    df1_dos['Source IP'].astype(str) + ':' +\n",
    "    df1_dos['Source Port'].astype(str) + '-' +\n",
    "    df1_dos['Destination IP'].astype(str) + ':' +\n",
    "    df1_dos['Destination Port'].astype(str) + '-' +\n",
    "    df1_dos['Protocol'].astype(str)\n",
    "    )\n",
    "df2['flow_key'] = (\n",
    "    df2['Src IP'].astype(str) + ':' +\n",
    "    df2['Src Port'].astype(str) + '-' +\n",
    "    df2['Dst IP'].astype(str) + ':' +\n",
    "    df2['Dst Port'].astype(str) + '-' +\n",
    "    df2['Protocol'].astype(str)\n",
    "    )\n",
    "df3['Protocol'] = df3['Protocol'].replace({'TCP': 6, 'UDP': 17, 'ICMP': 1}).astype(int)\n",
    "df3['flow_key'] = (\n",
    "    df3['Source IP'].astype(str) + ':' +\n",
    "    df3['Source Port'].astype(str) + '-' +\n",
    "    df3['Destination IP'].astype(str) + ':' +\n",
    "    df3['Destination Port'].astype(str) + '-' +\n",
    "    df3['Protocol'].astype(str)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "eb71a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique flow keys for slowhttptest from df1\n",
    "\n",
    "slowhttptest_flow_key = df1_dos['flow_key'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba510788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df2 and df3 for flows matching slowhttptest flow keys\n",
    "df2_dos = df2[df2['flow_key'].isin(slowhttptest_flow_key)]\n",
    "df3_dos = df3[df3['flow_key'].isin(slowhttptest_flow_key)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "08aad413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(659, 85)\n",
      "(376, 58)\n"
     ]
    }
   ],
   "source": [
    "print(df2_dos.shape)\n",
    "print(df3_dos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c14799d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "934"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df2['flow_key']).intersection(set(df3['flow_key'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d376f098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2078"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df2['flow_key']).difference(set(df3['flow_key'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f9158eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1504"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a9bfd29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1191, 58)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3_match_df2 = df3[df3['flow_key'].isin(df2['flow_key'])]\n",
    "df3_match_df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b23c7add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flow_key\n",
       "192.168.10.8:51422-72.21.91.29:80-6         4\n",
       "192.168.10.15:51282-172.217.10.130:443-6    4\n",
       "192.168.10.14:51229-72.21.91.29:80-6        4\n",
       "192.168.10.50:54722-192.168.10.3:389-6      4\n",
       "192.168.10.50:53708-192.168.10.3:3268-6     4\n",
       "                                           ..\n",
       "192.168.10.9:16149-35.160.9.151:443-6       1\n",
       "192.168.10.25:50569-89.36.26.156:443-6      1\n",
       "172.16.0.1:35048-192.168.10.50:80-6         1\n",
       "172.16.0.1:33860-192.168.10.50:80-6         1\n",
       "172.16.0.1:35028-192.168.10.50:80-6         1\n",
       "Name: count, Length: 934, dtype: int64"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3_match_df2['flow_key'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the above, from 3852 flow in df2, only 350 are DoS Slowhttptest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dada11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of flows: 659\n",
      "number of unique flows: 350\n"
     ]
    }
   ],
   "source": [
    "# finding duplication of flows indicated as attacks in df2\n",
    "slowhttptest_flows = df2[df2['Flow ID'].isin(set(df1_dos['Flow ID']))]\n",
    "print(\"number of flows:\", len(slowhttptest_flows['Flow ID']))\n",
    "print(\"number of unique flows:\", slowhttptest_flows['Flow ID'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2a7538",
   "metadata": {},
   "source": [
    "# Test model prediction on flow generated by CICFLOWMETER and from CICIDS2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f77d16da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wahba/miniconda3/envs/ml2/lib/python3.12/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator KNeighborsClassifier from version 1.2.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/wahba/miniconda3/envs/ml2/lib/python3.12/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator RobustScaler from version 1.2.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "rf = joblib.load(\"/home/wahba/Documents/models_anacletu/xgboost.joblib\")\n",
    "xgb = joblib.load(\"/home/wahba/Documents/models_anacletu/xgboost.joblib\")\n",
    "knn = joblib.load(\"/home/wahba/Documents/models_anacletu/knn_model.joblib\")\n",
    "robust_scaler = joblib.load(\"/home/wahba/Documents/models_anacletu/robust_scaler.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1850c45",
   "metadata": {},
   "source": [
    "# Standardizing dataframes to feed to ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c4270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing column names\n",
    "'''\n",
    "final_cols_df1 = {\n",
    "    'Source IP': 'Source IP',\n",
    "    'Destination IP': 'Destination IP',\n",
    "    'Protocol': 'Protocol',\n",
    "    'Source Port': 'Source Port',\n",
    "    'Destination Port': 'Destination Port',\n",
    "    'Flow Duration': 'Flow Duration',\n",
    "    'Total Fwd Packets': 'Total Fwd Packets',\n",
    "    'Total Length of Fwd Packets': 'Total Length of Fwd Packets',\n",
    "    'Fwd Packet Length Max': 'Fwd Packet Length Max',\n",
    "    'Fwd Packet Length Min': 'Fwd Packet Length Min',\n",
    "    'Fwd Packet Length Mean': 'Fwd Packet Length Mean',\n",
    "    'Fwd Packet Length Std': 'Fwd Packet Length Std',\n",
    "    'Bwd Packet Length Max': 'Bwd Packet Length Max',\n",
    "    'Bwd Packet Length Min': 'Bwd Packet Length Min',\n",
    "    'Bwd Packet Length Mean': 'Bwd Packet Length Mean',\n",
    "    'Bwd Packet Length Std': 'Bwd Packet Length Std',\n",
    "    'Flow Bytes/s': 'Flow Bytes/s',\n",
    "    'Flow Packets/s': 'Flow Packets/s',\n",
    "    'Flow IAT Mean': 'Flow IAT Mean',\n",
    "    'Flow IAT Std': 'Flow IAT Std',\n",
    "    'Flow IAT Max': 'Flow IAT Max',\n",
    "    'Flow IAT Min': 'Flow IAT Min',\n",
    "    'Fwd IAT Total': 'Fwd IAT Total',\n",
    "    'Fwd IAT Mean': 'Fwd IAT Mean',\n",
    "    'Fwd IAT Std': 'Fwd IAT Std',\n",
    "    'Fwd IAT Max': 'Fwd IAT Max',\n",
    "    'Fwd IAT Min': 'Fwd IAT Min',\n",
    "    'Bwd IAT Total': 'Bwd IAT Total',\n",
    "    'Bwd IAT Mean': 'Bwd IAT Mean',\n",
    "    'Bwd IAT Std': 'Bwd IAT Std',\n",
    "    'Bwd IAT Max': 'Bwd IAT Max',\n",
    "    'Bwd IAT Min': 'Bwd IAT Min',\n",
    "    'Fwd Header Length': 'Fwd Header Length',\n",
    "    'Bwd Header Length': 'Bwd Header Length',\n",
    "    'Fwd Packets/s': 'Fwd Packets/s',\n",
    "    'Bwd Packets/s': 'Bwd Packets/s',\n",
    "    'Min Packet Length': 'Min Packet Length',\n",
    "    'Max Packet Length': 'Max Packet Length',\n",
    "    'Packet Length Mean': 'Packet Length Mean',\n",
    "    'Packet Length Std': 'Packet Length Std',\n",
    "    'Packet Length Variance': 'Packet Length Variance',\n",
    "    'FIN Flag Count': 'FIN Flag Count',\n",
    "    'PSH Flag Count': 'PSH Flag Count',\n",
    "    'ACK Flag Count': 'ACK Flag Count',\n",
    "    'Average Packet Size': 'Average Packet Size',\n",
    "    'Subflow Fwd Bytes': 'Subflow Fwd Bytes',\n",
    "    'Init_Win_bytes_forward': 'Init_Win_bytes_forward',\n",
    "    'Init_Win_bytes_backward': 'Init_Win_bytes_backward',\n",
    "    'act_data_pkt_fwd': 'act_data_pkt_fwd',\n",
    "    'min_seg_size_forward': 'min_seg_size_forward',\n",
    "    'Active Mean': 'Active Mean',\n",
    "    'Active Max': 'Active Max',\n",
    "    'Active Min': 'Active Min',\n",
    "    'Idle Mean': 'Idle Mean',\n",
    "    'Idle Max': 'Idle Max',\n",
    "    'Idle Min': 'Idle Min'\n",
    "}\n",
    "\n",
    "final_cols_df2 = {\n",
    "    'Src IP': 'Source IP',\n",
    "    'Dst IP': 'Destination IP',\n",
    "    'Protocol': 'Protocol',\n",
    "    'Src Port': 'Source Port',\n",
    "    'Dst Port': 'Destination Port',\n",
    "    'Flow Duration': 'Flow Duration',\n",
    "    'Total Fwd Packet': 'Total Fwd Packets',\n",
    "    'Total Length of Fwd Packet': 'Total Length of Fwd Packets',\n",
    "    'Fwd Packet Length Max': 'Fwd Packet Length Max',\n",
    "    'Fwd Packet Length Min': 'Fwd Packet Length Min',\n",
    "    'Fwd Packet Length Mean': 'Fwd Packet Length Mean',\n",
    "    'Fwd Packet Length Std': 'Fwd Packet Length Std',\n",
    "    'Bwd Packet Length Max': 'Bwd Packet Length Max',\n",
    "    'Bwd Packet Length Min': 'Bwd Packet Length Min',\n",
    "    'Bwd Packet Length Mean': 'Bwd Packet Length Mean',\n",
    "    'Bwd Packet Length Std': 'Bwd Packet Length Std',\n",
    "    'Flow Bytes/s': 'Flow Bytes/s',\n",
    "    'Flow Packets/s': 'Flow Packets/s',\n",
    "    'Flow IAT Mean': 'Flow IAT Mean',\n",
    "    'Flow IAT Std': 'Flow IAT Std',\n",
    "    'Flow IAT Max': 'Flow IAT Max',\n",
    "    'Flow IAT Min': 'Flow IAT Min',\n",
    "    'Fwd IAT Total': 'Fwd IAT Total',\n",
    "    'Fwd IAT Mean': 'Fwd IAT Mean',\n",
    "    'Fwd IAT Std': 'Fwd IAT Std',\n",
    "    'Fwd IAT Max': 'Fwd IAT Max',\n",
    "    'Fwd IAT Min': 'Fwd IAT Min',\n",
    "    'Bwd IAT Total': 'Bwd IAT Total',\n",
    "    'Bwd IAT Mean': 'Bwd IAT Mean',\n",
    "    'Bwd IAT Std': 'Bwd IAT Std',\n",
    "    'Bwd IAT Max': 'Bwd IAT Max',\n",
    "    'Bwd IAT Min': 'Bwd IAT Min',\n",
    "    'Fwd Header Length': 'Fwd Header Length',\n",
    "    'Bwd Header Length': 'Bwd Header Length',\n",
    "    'Fwd Packets/s': 'Fwd Packets/s',\n",
    "    'Bwd Packets/s': 'Bwd Packets/s',\n",
    "    'Packet Length Min': 'Min Packet Length',\n",
    "    'Packet Length Max': 'Max Packet Length',\n",
    "    'Packet Length Mean': 'Packet Length Mean',\n",
    "    'Packet Length Std': 'Packet Length Std',\n",
    "    'Packet Length Variance': 'Packet Length Variance',\n",
    "    'FIN Flag Count': 'FIN Flag Count',\n",
    "    'PSH Flag Count': 'PSH Flag Count',\n",
    "    'ACK Flag Count': 'ACK Flag Count',\n",
    "    'Average Packet Size': 'Average Packet Size',\n",
    "    'Subflow Fwd Bytes': 'Subflow Fwd Bytes',\n",
    "    'FWD Init Win Bytes': 'Init_Win_bytes_forward',\n",
    "    'Bwd Init Win Bytes': 'Init_Win_bytes_backward',\n",
    "    'Fwd Act Data Pkts': 'act_data_pkt_fwd',\n",
    "    'Fwd Seg Size Min': 'min_seg_size_forward',\n",
    "    'Active Mean': 'Active Mean',\n",
    "    'Active Max': 'Active Max',\n",
    "    'Active Min': 'Active Min',\n",
    "    'Idle Mean': 'Idle Mean',\n",
    "    'Idle Max': 'Idle Max',\n",
    "    'Idle Min': 'Idle Min'\n",
    "}\n",
    "'''\n",
    "\n",
    "final_columns_df1 = {\n",
    "    'Destination Port': 'Destination Port',\n",
    "    'Flow Duration': 'Flow Duration',\n",
    "    'Total Fwd Packets': 'Total Fwd Packets',\n",
    "    'Total Length of Fwd Packets': 'Total Length of Fwd Packets',\n",
    "    'Fwd Packet Length Max': 'Fwd Packet Length Max',\n",
    "    'Fwd Packet Length Min': 'Fwd Packet Length Min',\n",
    "    'Fwd Packet Length Mean': 'Fwd Packet Length Mean',\n",
    "    'Fwd Packet Length Std': 'Fwd Packet Length Std',\n",
    "    'Bwd Packet Length Max': 'Bwd Packet Length Max',\n",
    "    'Bwd Packet Length Min': 'Bwd Packet Length Min',\n",
    "    'Bwd Packet Length Mean': 'Bwd Packet Length Mean',\n",
    "    'Bwd Packet Length Std': 'Bwd Packet Length Std',\n",
    "    'Flow Bytes/s': 'Flow Bytes/s',\n",
    "    'Flow Packets/s': 'Flow Packets/s',\n",
    "    'Flow IAT Mean': 'Flow IAT Mean',\n",
    "    'Flow IAT Std': 'Flow IAT Std',\n",
    "    'Flow IAT Max': 'Flow IAT Max',\n",
    "    'Flow IAT Min': 'Flow IAT Min',\n",
    "    'Fwd IAT Total': 'Fwd IAT Total',\n",
    "    'Fwd IAT Mean': 'Fwd IAT Mean',\n",
    "    'Fwd IAT Std': 'Fwd IAT Std',\n",
    "    'Fwd IAT Max': 'Fwd IAT Max',\n",
    "    'Fwd IAT Min': 'Fwd IAT Min',\n",
    "    'Bwd IAT Total': 'Bwd IAT Total',\n",
    "    'Bwd IAT Mean': 'Bwd IAT Mean',\n",
    "    'Bwd IAT Std': 'Bwd IAT Std',\n",
    "    'Bwd IAT Max': 'Bwd IAT Max',\n",
    "    'Bwd IAT Min': 'Bwd IAT Min',\n",
    "    'Fwd Header Length': 'Fwd Header Length',\n",
    "    'Bwd Header Length': 'Bwd Header Length',\n",
    "    'Fwd Packets/s': 'Fwd Packets/s',\n",
    "    'Bwd Packets/s': 'Bwd Packets/s',\n",
    "    'Min Packet Length': 'Min Packet Length',\n",
    "    'Max Packet Length': 'Max Packet Length',\n",
    "    'Packet Length Mean': 'Packet Length Mean',\n",
    "    'Packet Length Std': 'Packet Length Std',\n",
    "    'Packet Length Variance': 'Packet Length Variance',\n",
    "    'FIN Flag Count': 'FIN Flag Count',\n",
    "    'PSH Flag Count': 'PSH Flag Count',\n",
    "    'ACK Flag Count': 'ACK Flag Count',\n",
    "    'Average Packet Size': 'Average Packet Size',\n",
    "    'Subflow Fwd Bytes': 'Subflow Fwd Bytes',\n",
    "    'Init_Win_bytes_forward': 'Init_Win_bytes_forward',\n",
    "    'Init_Win_bytes_backward': 'Init_Win_bytes_backward',\n",
    "    'act_data_pkt_fwd': 'act_data_pkt_fwd',\n",
    "    'min_seg_size_forward': 'min_seg_size_forward',\n",
    "    'Active Mean': 'Active Mean',\n",
    "    'Active Max': 'Active Max',\n",
    "    'Active Min': 'Active Min',\n",
    "    'Idle Mean': 'Idle Mean',\n",
    "    'Idle Max': 'Idle Max',\n",
    "    'Idle Min': 'Idle Min'\n",
    "}\n",
    "\n",
    "final_columns_df2 = {\n",
    "    'Dst Port': 'Destination Port',\n",
    "    'Flow Duration': 'Flow Duration',\n",
    "    'Total Fwd Packet': 'Total Fwd Packets',\n",
    "    'Total Length of Fwd Packet': 'Total Length of Fwd Packets',\n",
    "    'Fwd Packet Length Max': 'Fwd Packet Length Max',\n",
    "    'Fwd Packet Length Min': 'Fwd Packet Length Min',\n",
    "    'Fwd Packet Length Mean': 'Fwd Packet Length Mean',\n",
    "    'Fwd Packet Length Std': 'Fwd Packet Length Std',\n",
    "    'Bwd Packet Length Max': 'Bwd Packet Length Max',\n",
    "    'Bwd Packet Length Min': 'Bwd Packet Length Min',\n",
    "    'Bwd Packet Length Mean': 'Bwd Packet Length Mean',\n",
    "    'Bwd Packet Length Std': 'Bwd Packet Length Std',\n",
    "    'Flow Bytes/s': 'Flow Bytes/s',\n",
    "    'Flow Packets/s': 'Flow Packets/s',\n",
    "    'Flow IAT Mean': 'Flow IAT Mean',\n",
    "    'Flow IAT Std': 'Flow IAT Std',\n",
    "    'Flow IAT Max': 'Flow IAT Max',\n",
    "    'Flow IAT Min': 'Flow IAT Min',\n",
    "    'Fwd IAT Total': 'Fwd IAT Total',\n",
    "    'Fwd IAT Mean': 'Fwd IAT Mean',\n",
    "    'Fwd IAT Std': 'Fwd IAT Std',\n",
    "    'Fwd IAT Max': 'Fwd IAT Max',\n",
    "    'Fwd IAT Min': 'Fwd IAT Min',\n",
    "    'Bwd IAT Total': 'Bwd IAT Total',\n",
    "    'Bwd IAT Mean': 'Bwd IAT Mean',\n",
    "    'Bwd IAT Std': 'Bwd IAT Std',\n",
    "    'Bwd IAT Max': 'Bwd IAT Max',\n",
    "    'Bwd IAT Min': 'Bwd IAT Min',\n",
    "    'Fwd Header Length': 'Fwd Header Length',\n",
    "    'Bwd Header Length': 'Bwd Header Length',\n",
    "    'Fwd Packets/s': 'Fwd Packets/s',\n",
    "    'Bwd Packets/s': 'Bwd Packets/s',\n",
    "    'Packet Length Min': 'Min Packet Length',\n",
    "    'Packet Length Max': 'Max Packet Length',\n",
    "    'Packet Length Mean': 'Packet Length Mean',\n",
    "    'Packet Length Std': 'Packet Length Std',\n",
    "    'Packet Length Variance': 'Packet Length Variance',\n",
    "    'FIN Flag Count': 'FIN Flag Count',\n",
    "    'PSH Flag Count': 'PSH Flag Count',\n",
    "    'ACK Flag Count': 'ACK Flag Count',\n",
    "    'Average Packet Size': 'Average Packet Size',\n",
    "    'Subflow Fwd Bytes': 'Subflow Fwd Bytes',\n",
    "    'FWD Init Win Bytes': 'Init_Win_bytes_forward',\n",
    "    'Bwd Init Win Bytes': 'Init_Win_bytes_backward',\n",
    "    'Fwd Act Data Pkts': 'act_data_pkt_fwd',\n",
    "    'Fwd Seg Size Min': 'min_seg_size_forward',\n",
    "    'Active Mean': 'Active Mean',\n",
    "    'Active Max': 'Active Max',\n",
    "    'Active Min': 'Active Min',\n",
    "    'Idle Mean': 'Idle Mean',\n",
    "    'Idle Max': 'Idle Max',\n",
    "    'Idle Min': 'Idle Min'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "098ea70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected number of features for the model: 52\n"
     ]
    }
   ],
   "source": [
    "df1_dos_standardized = df1_dos[list(final_columns_df1.keys())].rename(columns=final_columns_df1)\n",
    "df2_standardized = df2[list(final_columns_df2.keys())].rename(columns=final_columns_df2)\n",
    "df3_match_df2_standardized = df3_match_df2[list(final_columns_df1.keys())].rename(columns=final_columns_df1)\n",
    "print(\"expected number of features for the model:\", len(final_columns_df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "67387c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1_dos_standardized shape: (5499, 52)\n",
      "df2_standardized shape: (3852, 52)\n",
      "df3_match_df2_standardized shape: (1191, 52)\n",
      "Expected number of features for the model: 52\n"
     ]
    }
   ],
   "source": [
    "print(\"df1_dos_standardized shape:\", df1_dos_standardized.shape)\n",
    "print(\"df2_standardized shape:\", df2_standardized.shape)\n",
    "print(\"df3_match_df2_standardized shape:\", df3_match_df2_standardized.shape)\n",
    "print(\"Expected number of features for the model:\", len(final_columns_df1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a78ca74",
   "metadata": {},
   "source": [
    "# Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "de09e9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wahba/miniconda3/envs/ml2/lib/python3.12/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator KNeighborsClassifier from version 1.2.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/wahba/miniconda3/envs/ml2/lib/python3.12/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator RobustScaler from version 1.2.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "rf = joblib.load(\"/home/wahba/Documents/models_anacletu/xgboost.joblib\")\n",
    "xgb = joblib.load(\"/home/wahba/Documents/models_anacletu/xgboost.joblib\")\n",
    "knn = joblib.load(\"/home/wahba/Documents/models_anacletu/knn_model.joblib\")\n",
    "robust_scaler = joblib.load(\"/home/wahba/Documents/models_anacletu/robust_scaler.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "8f73263e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6]\n",
      "[0 1 2 3 4 5 6]\n",
      "['Bots' 'Brute Force' 'DDoS' 'DoS' 'Normal Traffic' 'Port Scanning'\n",
      " 'Web Attacks']\n"
     ]
    }
   ],
   "source": [
    "print(rf.classes_)\n",
    "print(xgb.classes_)\n",
    "print(knn.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "06a5d131",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions_df1 = rf.predict_proba(df1_dos_standardized)\n",
    "xgb_predictions_df1 = xgb.predict_proba(df1_dos_standardized)\n",
    "knn_predictions_df1 = knn.predict_proba(robust_scaler.transform(df1_dos_standardized))\n",
    "\n",
    "rf_predictions_df2 = rf.predict_proba(df2_standardized)\n",
    "xgb_predictions_df2 = xgb.predict_proba(df2_standardized)\n",
    "knn_predictions_df2 = knn.predict_proba(robust_scaler.transform(df2_standardized))\n",
    "\n",
    "rf_predictions_df3 = rf.predict_proba(df3_match_df2_standardized)\n",
    "xgb_predictions_df3 = xgb.predict_proba(df3_match_df2_standardized)\n",
    "knn_predictions_df3 = knn.predict_proba(robust_scaler.transform(df3_match_df2_standardized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "5b55c9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Count                                          Percentage  \\\n",
      "Class               0       1     2       3       4     5    6          0   \n",
      "Dataset Model                                                               \n",
      "df1     KNN       NaN     NaN   1.0  5308.0   189.0   NaN  1.0        NaN   \n",
      "        RF        4.0  5493.0   NaN     1.0     NaN   1.0  NaN       0.1%   \n",
      "        XGB       4.0  5493.0   NaN     1.0     NaN   1.0  NaN       0.1%   \n",
      "df2     KNN       NaN     1.0  41.0   434.0  3359.0  17.0  NaN        NaN   \n",
      "        RF     3193.0   659.0   NaN     NaN     NaN   NaN  NaN      82.9%   \n",
      "        XGB    3193.0   659.0   NaN     NaN     NaN   NaN  NaN      82.9%   \n",
      "df3     KNN       NaN     NaN   8.0   281.0   902.0   NaN  NaN        NaN   \n",
      "        RF      989.0   202.0   NaN     NaN     NaN   NaN  NaN      83.0%   \n",
      "        XGB     989.0   202.0   NaN     NaN     NaN   NaN  NaN      83.0%   \n",
      "\n",
      "                                                      \n",
      "Class              1     2      3      4     5     6  \n",
      "Dataset Model                                         \n",
      "df1     KNN      NaN  0.0%  96.5%   3.4%   NaN  0.0%  \n",
      "        RF     99.9%   NaN   0.0%    NaN  0.0%   NaN  \n",
      "        XGB    99.9%   NaN   0.0%    NaN  0.0%   NaN  \n",
      "df2     KNN     0.0%  1.1%  11.3%  87.2%  0.4%   NaN  \n",
      "        RF     17.1%   NaN    NaN    NaN   NaN   NaN  \n",
      "        XGB    17.1%   NaN    NaN    NaN   NaN   NaN  \n",
      "df3     KNN      NaN  0.7%  23.6%  75.7%   NaN   NaN  \n",
      "        RF     17.0%   NaN    NaN    NaN   NaN   NaN  \n",
      "        XGB    17.0%   NaN    NaN    NaN   NaN   NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a summary dataframe\n",
    "results = []\n",
    "\n",
    "datasets = ['df1', 'df2', 'df3']\n",
    "models = ['rf', 'xgb', 'knn']\n",
    "predictions_dict = {\n",
    "    'df1': {'rf': rf_predictions_df1, 'xgb': xgb_predictions_df1, 'knn': knn_predictions_df1},\n",
    "    'df2': {'rf': rf_predictions_df2, 'xgb': xgb_predictions_df2, 'knn': knn_predictions_df2},\n",
    "    'df3': {'rf': rf_predictions_df3, 'xgb': xgb_predictions_df3, 'knn': knn_predictions_df3}\n",
    "}\n",
    "\n",
    "for dataset in datasets:\n",
    "    for model in models:\n",
    "        preds = predictions_dict[dataset][model]\n",
    "        value_counts = pd.Series(preds.argmax(axis=1) if len(preds.shape) > 1 else preds).value_counts().sort_index()\n",
    "        \n",
    "        for class_label, count in value_counts.items():\n",
    "            results.append({\n",
    "                'Dataset': dataset,\n",
    "                'Model': model.upper(),\n",
    "                'Class': class_label,\n",
    "                'Count': count,\n",
    "                'Percentage': f\"{(count/len(preds)*100):.1f}%\"\n",
    "            })\n",
    "\n",
    "summary_df = pd.DataFrame(results)\n",
    "print(summary_df.pivot_table(index=['Dataset', 'Model'], columns='Class', values=['Count', 'Percentage'], aggfunc='first'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "18e19612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DATASET: DF1\n",
      "==================================================\n",
      "\n",
      "RF Predictions:\n",
      "  Class 0: 4 (0.1%)\n",
      "  Class 1: 5493 (99.9%)\n",
      "  Class 3: 1 (0.0%)\n",
      "  Class 5: 1 (0.0%)\n",
      "\n",
      "XGB Predictions:\n",
      "  Class 0: 4 (0.1%)\n",
      "  Class 1: 5493 (99.9%)\n",
      "  Class 3: 1 (0.0%)\n",
      "  Class 5: 1 (0.0%)\n",
      "\n",
      "KNN Predictions:\n",
      "  Class 2: 1 (0.0%)\n",
      "  Class 3: 5308 (96.5%)\n",
      "  Class 4: 189 (3.4%)\n",
      "  Class 6: 1 (0.0%)\n",
      "\n",
      "==================================================\n",
      "DATASET: DF2\n",
      "==================================================\n",
      "\n",
      "RF Predictions:\n",
      "  Class 0: 3193 (82.9%)\n",
      "  Class 1: 659 (17.1%)\n",
      "\n",
      "XGB Predictions:\n",
      "  Class 0: 3193 (82.9%)\n",
      "  Class 1: 659 (17.1%)\n",
      "\n",
      "KNN Predictions:\n",
      "  Class 1: 1 (0.0%)\n",
      "  Class 2: 41 (1.1%)\n",
      "  Class 3: 434 (11.3%)\n",
      "  Class 4: 3359 (87.2%)\n",
      "  Class 5: 17 (0.4%)\n",
      "\n",
      "==================================================\n",
      "DATASET: DF3\n",
      "==================================================\n",
      "\n",
      "RF Predictions:\n",
      "  Class 0: 989 (83.0%)\n",
      "  Class 1: 202 (17.0%)\n",
      "\n",
      "XGB Predictions:\n",
      "  Class 0: 989 (83.0%)\n",
      "  Class 1: 202 (17.0%)\n",
      "\n",
      "KNN Predictions:\n",
      "  Class 2: 8 (0.7%)\n",
      "  Class 3: 281 (23.6%)\n",
      "  Class 4: 902 (75.7%)\n"
     ]
    }
   ],
   "source": [
    "def display_predictions_summary(predictions_dict, dataset_names=None, model_names=None):\n",
    "    if dataset_names is None:\n",
    "        dataset_names = list(predictions_dict.keys())\n",
    "    if model_names is None:\n",
    "        model_names = list(predictions_dict[dataset_names[0]].keys())\n",
    "    \n",
    "    for dataset in dataset_names:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"DATASET: {dataset.upper()}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        for model in model_names:\n",
    "            preds = predictions_dict[dataset][model]\n",
    "            # Handle both binary and multiclass probability arrays\n",
    "            if len(preds.shape) > 1:\n",
    "                class_predictions = preds.argmax(axis=1)\n",
    "            else:\n",
    "                class_predictions = (preds > 0.5).astype(int)\n",
    "            \n",
    "            value_counts = pd.Series(class_predictions).value_counts().sort_index()\n",
    "            \n",
    "            print(f\"\\n{model.upper()} Predictions:\")\n",
    "            for class_label, count in value_counts.items():\n",
    "                percentage = (count / len(preds)) * 100\n",
    "                print(f\"  Class {class_label}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Usage\n",
    "predictions_dict = {\n",
    "    'df1': {'rf': rf_predictions_df1, 'xgb': xgb_predictions_df1, 'knn': knn_predictions_df1},\n",
    "    'df2': {'rf': rf_predictions_df2, 'xgb': xgb_predictions_df2, 'knn': knn_predictions_df2},\n",
    "    'df3': {'rf': rf_predictions_df3, 'xgb': xgb_predictions_df3, 'knn': knn_predictions_df3}\n",
    "}\n",
    "\n",
    "display_predictions_summary(predictions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "20e7bb84",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data must be 1-dimensional, got ndarray of shape (5499, 7) instead",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[194]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m rf_predictions_df1_pd = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrf_predictions_df1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m xgb_predictions_df1_pd = pd.Series(xgb_predictions_df1)\n\u001b[32m      3\u001b[39m knn_predictions_df1_pd = pd.Series(knn_predictions_df1)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml2/lib/python3.12/site-packages/pandas/core/series.py:587\u001b[39m, in \u001b[36mSeries.__init__\u001b[39m\u001b[34m(self, data, index, dtype, name, copy, fastpath)\u001b[39m\n\u001b[32m    585\u001b[39m         data = data.copy()\n\u001b[32m    586\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m587\u001b[39m     data = \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    589\u001b[39m     manager = _get_option(\u001b[33m\"\u001b[39m\u001b[33mmode.data_manager\u001b[39m\u001b[33m\"\u001b[39m, silent=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    590\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33mblock\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml2/lib/python3.12/site-packages/pandas/core/construction.py:656\u001b[39m, in \u001b[36msanitize_array\u001b[39m\u001b[34m(data, index, dtype, copy, allow_2d)\u001b[39m\n\u001b[32m    653\u001b[39m             subarr = cast(np.ndarray, subarr)\n\u001b[32m    654\u001b[39m             subarr = maybe_infer_to_datetimelike(subarr)\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m subarr = \u001b[43m_sanitize_ndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_2d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(subarr, np.ndarray):\n\u001b[32m    659\u001b[39m     \u001b[38;5;66;03m# at this point we should have dtype be None or subarr.dtype == dtype\u001b[39;00m\n\u001b[32m    660\u001b[39m     dtype = cast(np.dtype, dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml2/lib/python3.12/site-packages/pandas/core/construction.py:715\u001b[39m, in \u001b[36m_sanitize_ndim\u001b[39m\u001b[34m(result, data, dtype, index, allow_2d)\u001b[39m\n\u001b[32m    713\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m allow_2d:\n\u001b[32m    714\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m--> \u001b[39m\u001b[32m715\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    716\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mData must be 1-dimensional, got ndarray of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m instead\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    717\u001b[39m     )\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[32m    719\u001b[39m     \u001b[38;5;66;03m# i.e. NumpyEADtype(\"O\")\u001b[39;00m\n\u001b[32m    721\u001b[39m     result = com.asarray_tuplesafe(data, dtype=np.dtype(\u001b[33m\"\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mValueError\u001b[39m: Data must be 1-dimensional, got ndarray of shape (5499, 7) instead"
     ]
    }
   ],
   "source": [
    "rf_predictions_df1_pd = pd.Series(rf_predictions_df1)\n",
    "xgb_predictions_df1_pd = pd.Series(xgb_predictions_df1)\n",
    "knn_predictions_df1_pd = pd.Series(knn_predictions_df1)\n",
    "\n",
    "rf_predictions_df2_pd = pd.Series(rf_predictions_df2)\n",
    "xgb_predictions_df2_pd = pd.Series(xgb_predictions_df2)\n",
    "knn_predictions_df2_pd = pd.Series(knn_predictions_df2)\n",
    "\n",
    "rf_predictions_df3_pd = pd.Series(rf_predictions_df3)\n",
    "xgb_predictions_df3_pd = pd.Series(xgb_predictions_df3)\n",
    "knn_predictions_df3_pd = pd.Series(knn_predictions_df3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "54dca659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf preditions df1\n",
      " 1    5493\n",
      "0       4\n",
      "3       1\n",
      "5       1\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "xgb preditions df1\n",
      " 1    5493\n",
      "0       4\n",
      "3       1\n",
      "5       1\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "knn preditions df1\n",
      " DoS               5308\n",
      "Normal Traffic     189\n",
      "Web Attacks          1\n",
      "DDoS                 1\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "rf preditions df2\n",
      " 0    3193\n",
      "1     659\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "xgb preditions df2\n",
      " 0    3193\n",
      "1     659\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "knn preditions df2\n",
      " Normal Traffic    3359\n",
      "DoS                434\n",
      "DDoS                41\n",
      "Port Scanning       17\n",
      "Brute Force          1\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "rf preditions df3\n",
      " 0    989\n",
      "1    202\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "xgb preditions df3\n",
      " 0    989\n",
      "1    202\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "knn preditions df3\n",
      " Normal Traffic    902\n",
      "DoS               281\n",
      "DDoS                8\n",
      "Name: count, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"rf preditions df1\\n\",rf_predictions_df1_pd.value_counts(), \"\\n\")\n",
    "print(\"xgb preditions df1\\n\",xgb_predictions_df1_pd.value_counts(), \"\\n\")\n",
    "print(\"knn preditions df1\\n\",knn_predictions_df1_pd.value_counts(), \"\\n\")\n",
    "print(\"rf preditions df2\\n\",rf_predictions_df2_pd.value_counts(), \"\\n\")\n",
    "print(\"xgb preditions df2\\n\",xgb_predictions_df2_pd.value_counts(), \"\\n\")\n",
    "print(\"knn preditions df2\\n\",knn_predictions_df2_pd.value_counts(), \"\\n\")\n",
    "print(\"rf preditions df3\\n\",rf_predictions_df3_pd.value_counts(), \"\\n\")\n",
    "print(\"xgb preditions df3\\n\",xgb_predictions_df3_pd.value_counts(), \"\\n\")\n",
    "print(\"knn preditions df3\\n\",knn_predictions_df3_pd.value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "859a9a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATASET: DF1\n",
      "============================================================\n",
      "RF    : Avg Probability = 0.997 ± 0.024 (High Confidence: 99.9%)\n",
      "XGB   : Avg Probability = 0.997 ± 0.024 (High Confidence: 99.9%)\n",
      "KNN   : Avg Probability = 0.995 ± 0.038 (High Confidence: 100.0%)\n",
      "\n",
      "============================================================\n",
      "DATASET: DF2\n",
      "============================================================\n",
      "RF    : Avg Probability = 0.999 ± 0.011 (High Confidence: 100.0%)\n",
      "XGB   : Avg Probability = 0.999 ± 0.011 (High Confidence: 100.0%)\n",
      "KNN   : Avg Probability = 0.987 ± 0.068 (High Confidence: 100.0%)\n",
      "\n",
      "============================================================\n",
      "DATASET: DF3\n",
      "============================================================\n",
      "RF    : Avg Probability = 0.983 ± 0.051 (High Confidence: 100.0%)\n",
      "XGB   : Avg Probability = 0.983 ± 0.051 (High Confidence: 100.0%)\n",
      "KNN   : Avg Probability = 0.985 ± 0.069 (High Confidence: 100.0%)\n"
     ]
    }
   ],
   "source": [
    "def display_average_probabilities(predictions_dict):\n",
    "    \"\"\"\n",
    "    Display average probability of the predicted class for each model and dataset\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for dataset_name, models in predictions_dict.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"DATASET: {dataset_name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for model_name, predictions in models.items():\n",
    "            # Get the predicted class and its probability\n",
    "            if len(predictions.shape) > 1:  # Multiclass probability array\n",
    "                predicted_classes = predictions.argmax(axis=1)\n",
    "                predicted_probs = predictions[np.arange(len(predictions)), predicted_classes]\n",
    "            else:  # Binary classification probabilities\n",
    "                predicted_classes = (predictions > 0.5).astype(int)\n",
    "                predicted_probs = np.where(predicted_classes == 1, predictions, 1 - predictions)\n",
    "            \n",
    "            avg_prob = predicted_probs.mean()\n",
    "            std_prob = predicted_probs.std()\n",
    "            confidence_ratio = (predicted_probs > 0.5).mean()  # % of predictions with >50% confidence\n",
    "            \n",
    "            results.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Model': model_name.upper(),\n",
    "                'Avg_Probability': avg_prob,\n",
    "                'Std_Probability': std_prob,\n",
    "                'High_Confidence_Ratio': confidence_ratio,\n",
    "                'Samples': len(predictions)\n",
    "            })\n",
    "            \n",
    "            print(f\"{model_name.upper():<6}: Avg Probability = {avg_prob:.3f} ± {std_prob:.3f} \"\n",
    "                  f\"(High Confidence: {confidence_ratio:.1%})\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Usage\n",
    "predictions_dict = {\n",
    "    'df1': {'rf': rf_predictions_df1, 'xgb': xgb_predictions_df1, 'knn': knn_predictions_df1},\n",
    "    'df2': {'rf': rf_predictions_df2, 'xgb': xgb_predictions_df2, 'knn': knn_predictions_df2},\n",
    "    'df3': {'rf': rf_predictions_df3, 'xgb': xgb_predictions_df3, 'knn': knn_predictions_df3}\n",
    "}\n",
    "\n",
    "summary_df = display_average_probabilities(predictions_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
